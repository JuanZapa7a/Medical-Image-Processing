{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FYrUe-AtjBZC"
      },
      "outputs": [],
      "source": [
        "%pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4TES2xHFjBZD",
        "outputId": "021a3327-1300-466f-ec8f-946300d31518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b25028b5-5704-431f-bed0-95ad8e42df68\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b25028b5-5704-431f-bed0-95ad8e42df68\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xdmrN8FNjBZD",
        "outputId": "0ec33a24-cfd0-4717-d7e1-a6f3aa131ad6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 72\n",
            "drwx------ 1 root root 4096 Jul 24 07:03 .\n",
            "drwxr-xr-x 1 root root 4096 Jul 24 07:01 ..\n",
            "-r-xr-xr-x 1 root root 1169 Jan  1  2000 .bashrc\n",
            "drwxr-xr-x 1 root root 4096 Jul 22 13:40 .cache\n",
            "drwxr-xr-x 3 root root 4096 Jul 22 13:40 .config\n",
            "drwxr-xr-x 5 root root 4096 Jul 22 13:40 .ipython\n",
            "drwx------ 1 root root 4096 Jul 22 13:40 .jupyter\n",
            "drwxr-xr-x 2 root root 4096 Jul 24 07:03 .kaggle\n",
            "drwxr-xr-x 2 root root 4096 Jul 24 07:02 .keras\n",
            "drwx------ 3 root root 4096 Jul 22 13:14 .launchpadlib\n",
            "drwxr-xr-x 1 root root 4096 Jul 22 13:40 .local\n",
            "drwxr-xr-x 4 root root 4096 Jul 22 13:40 .npm\n",
            "-rw-r--r-- 1 root root  161 Jul  9  2019 .profile\n",
            "-r-xr-xr-x 1 root root  254 Jan  1  2000 .tmux.conf\n",
            "-rw-r--r-- 1 root root  211 Jul 22 13:40 .wget-hsts\n"
          ]
        }
      ],
      "source": [
        "%mkdir -p ~/.kaggle\n",
        "%cp ./kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!ls -al ~"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets list"
      ],
      "metadata": {
        "id": "Kf9WQIsHjg6g",
        "outputId": "f3273fa0-2a9f-458f-ee67-f88285a17423",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                         title                                                size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "----------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "rabieelkharoua/students-performance-dataset                 📚 Students Performance Dataset 📚                     66KB  2024-06-12 23:09:20          19900        438  1.0              \n",
            "nelgiriyewithana/most-streamed-spotify-songs-2024           Most Streamed Spotify Songs 2024                    496KB  2024-06-15 18:50:51          14856        316  1.0              \n",
            "ihelon/coffee-sales                                         Coffee Sales                                         11KB  2024-07-18 10:06:43           5940         98  1.0              \n",
            "abdullahshf/neet-ug-2024-results-all-india                  NEET UG 2024 Results - All India                     12MB  2024-07-21 16:30:14            524         26  1.0              \n",
            "priyamchoksi/100000-diabetes-clinical-dataset               Comprehensive Diabetes Clinical Dataset(100k rows)  896KB  2024-07-20 15:11:02            541         23  1.0              \n",
            "humairmunir/lung-cancer-risk-dataset                        Lung Cancer Dataset                                  21KB  2024-07-17 14:25:57            795         28  1.0              \n",
            "adarshde/electric-vehicle-population-dataset                Electric Vehicle Population DataSet                   6MB  2024-07-17 17:17:38           1619         32  1.0              \n",
            "priyamchoksi/adult-census-income-dataset                    Adult Census Income Dataset                         450KB  2024-07-07 17:01:37            697         30  1.0              \n",
            "fahmidachowdhury/e-commerce-sales-analysis                  📈 E-Commerce Sales Analysis                          35KB  2024-07-04 20:02:23           2820         51  0.9411765        \n",
            "utsavdey1410/food-nutrition-dataset                         Food Nutrition Dataset                              694KB  2024-06-29 19:42:01           3390         88  1.0              \n",
            "ivansher/nasa-nearest-earth-objects-1910-2024               NASA | Nearest Earth Objects (1910-2024)             13MB  2024-07-18 13:17:11            777         28  1.0              \n",
            "dataanalyst001/world-population-growth-rate-by-cities-2024  World population growth rate by cities 2024          16KB  2024-07-07 09:04:40            423         17  1.0              \n",
            "adityabhaumik/ipl-2024-matches                              IPL 2024 Matches                                      3KB  2024-07-10 18:18:03            752         22  1.0              \n",
            "suchintikasarkar/sentiment-analysis-for-mental-health       Sentiment Analysis for Mental Health                 11MB  2024-07-05 13:58:31           1472         44  1.0              \n",
            "dataanalyst001/world-literacy-rate-by-country               World Literacy Rate by Country                        2KB  2024-07-15 08:28:28           1038         24  1.0              \n",
            "preethamgouda/campaign-data                                 campaign_data                                         3MB  2024-07-19 07:34:22            321         31  1.0              \n",
            "stacknishant/nse-stock-historical-price-data                NSE Stock Historical price data                      20MB  2024-07-11 10:58:10           1509         29  1.0              \n",
            "zahrayazdani81/carsdataset                                  CarsDataset                                           4KB  2024-07-01 16:25:36           1315         22  0.7058824        \n",
            "krishd123/uefa-euro-2024-records                            ⚽UEFA Euro 2024 Records🏆                              5KB  2024-07-17 04:33:13            596         28  1.0              \n",
            "dansbecker/melbourne-housing-snapshot                       Melbourne Housing Snapshot                          451KB  2018-06-05 12:52:24         152665       1505  0.7058824        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets list -s \"breast\""
      ],
      "metadata": {
        "id": "c0DosuVpjpLF",
        "outputId": "8c2aef8c-394b-4c50-bb84-3dfa66bc2f7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                              title                                           size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "---------------------------------------------------------------  ---------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "reihanenamdari/breast-cancer                                     Breast Cancer                                   43KB  2022-08-08 19:25:55          21458        277  1.0              \n",
            "yasserh/breast-cancer-dataset                                    Breast Cancer Dataset                           49KB  2021-12-29 19:07:20          58152        441  1.0              \n",
            "paultimothymooney/breast-histopathology-images                   Breast Histopathology Images                     3GB  2017-12-19 05:46:40          64452        997  0.75             \n",
            "imtkaggleteam/breast-cancer                                      Breast Cancer                                   49KB  2023-10-21 19:19:28           2121         98  1.0              \n",
            "nancyalaswad90/breast-cancer-dataset                             Breast Cancer Dataset                           49KB  2022-06-17 12:33:29          12343        213  0.9411765        \n",
            "aryashah2k/breast-ultrasound-images-dataset                      Breast Ultrasound Images Dataset               195MB  2021-03-14 04:29:54          31514        333  1.0              \n",
            "uciml/breast-cancer-wisconsin-data                               Breast Cancer Wisconsin (Diagnostic) Data Set   49KB  2016-09-25 10:49:04         375001       3558  0.85294116       \n",
            "piotrgrabo/breastcancerproteomes                                 Breast Cancer Proteomes                          5MB  2019-11-14 05:15:12          14735        365  0.64705884       \n",
            "merishnasuwal/breast-cancer-prediction-dataset                   Breast Cancer Prediction Dataset                 8KB  2018-09-26 12:41:51          30963        272  0.8235294        \n",
            "marshuu/breast-cancer                                            Breast cancer (cleaned)                          3KB  2023-01-08 09:22:50           4103         58  1.0              \n",
            "utkarshx27/breast-cancer-dataset-used-royston-and-altman         Breast Cancer Dataset                           10KB  2023-05-09 10:42:51           2549         54  1.0              \n",
            "vijayaadithyanvg/breast-cancer-prediction                        Breast Cancer Prediction                        49KB  2022-09-22 09:57:09           3000         54  0.9705882        \n",
            "faysalmiah1721758/breast-cancer-data                             Breast Cancer Dataset                            2KB  2023-08-14 06:01:43           1595         48  1.0              \n",
            "amandam1/breastcancerdataset                                     Real Breast Cancer Data                         11KB  2021-08-05 17:58:17           8271        101  1.0              \n",
            "simjeg/lymphoma-subtype-classification-fl-vs-cll                 Breast Histology Images                         33MB  2017-05-13 09:14:28           6017         65  0.75             \n",
            "krupadharamshi/breast-cancer-dataset                             Breast Cancer Dataset                           49KB  2024-06-16 06:59:35           1043         29  1.0              \n",
            "fatemehmehrparvar/breast-cancer-prediction                       Breast Cancer Prediction                         2KB  2024-01-16 16:23:39           1231         56  0.7647059        \n",
            "adhamelkomy/breast-cancer                                        Breast Cancer                                   49KB  2024-03-01 20:13:17            927         30  0.7058824        \n",
            "vuppalaadithyasairam/ultrasound-breast-images-for-breast-cancer  Ultrasound Breast Images for Breast Cancer     564MB  2022-11-03 05:51:50           2195         50  0.75             \n",
            "brunogrisci/breast-cancer-gene-expression-cumida                 Breast cancer gene expression - CuMiDa          62MB  2020-02-01 10:51:48           3910         81  0.9705882        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d aryashah2k/breast-ultrasound-images-dataset --unzip --force"
      ],
      "metadata": {
        "id": "uP2hIoggkgr1",
        "outputId": "6175931e-8235-4527-f978-ffb2fbd220bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/aryashah2k/breast-ultrasound-images-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading breast-ultrasound-images-dataset.zip to /content\n",
            " 92% 179M/195M [00:01<00:00, 144MB/s]\n",
            "100% 195M/195M [00:01<00:00, 150MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Pip Tree globally\n",
        "\n",
        "!sudo apt install tree -q\n",
        "\n",
        "\n",
        "!tree --dirsfirst -L 1  \"Dataset_BUSI_with_GT\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYz1hbbOkpPZ",
        "outputId": "1f08830d-b74e-4a49-afb3-f190e7df284b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "tree is already the newest version (2.0.2-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "\u001b[01;34mDataset_BUSI_with_GT\u001b[0m\n",
            "├── \u001b[01;34mbenign\u001b[0m\n",
            "├── \u001b[01;34mmalignant\u001b[0m\n",
            "└── \u001b[01;34mnormal\u001b[0m\n",
            "\n",
            "3 directories, 0 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Pip Tree globally\n",
        "\n",
        "!sudo apt install tree -q\n",
        "\n",
        "\n",
        "!tree --dirsfirst -L 2  \"Dataset_BUSI_with_GT\"| head -n 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4341386b-94fe-4bf9-d250-ef355650b7ad",
        "id": "50dS2-Kjs1jM"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "tree is already the newest version (2.0.2-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Dataset_BUSI_with_GT\n",
            "├── benign\n",
            "│   ├── benign (100)_mask_1.png\n",
            "│   ├── benign (100)_mask.png\n",
            "│   ├── benign (100).png\n",
            "│   ├── benign (101)_mask.png\n",
            "│   ├── benign (101).png\n",
            "│   ├── benign (102)_mask.png\n",
            "│   ├── benign (102).png\n",
            "│   ├── benign (103)_mask.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the paths\n",
        "original_dataset_dir = 'Dataset_BUSI_with_GT'\n",
        "base_dir = 'BreastUltraSoundImages'\n",
        "\n",
        "# Define new paths for train, val, and test\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# Create directories if they do not exist\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# List the classes\n",
        "classes = ['benign', 'malignant', 'normal']\n",
        "\n",
        "# Function to copy images\n",
        "def copy_images(class_name, src_dir, train_dst, val_dst, test_dst, train_ratio=0.7, val_ratio=0.15):\n",
        "    # List all images in the class\n",
        "    images = os.listdir(src_dir)\n",
        "    images = [os.path.join(src_dir, img) for img in images]\n",
        "\n",
        "    # Split into train, val, and test\n",
        "    train_images, temp_images = train_test_split(images, train_size=train_ratio, random_state=42)\n",
        "    val_images, test_images = train_test_split(temp_images, test_size=val_ratio/(val_ratio + (1 - train_ratio)), random_state=42)\n",
        "\n",
        "    # Create class directories in train, val, and test\n",
        "    train_class_dir = os.path.join(train_dst, class_name)\n",
        "    val_class_dir = os.path.join(val_dst, class_name)\n",
        "    test_class_dir = os.path.join(test_dst, class_name)\n",
        "\n",
        "    # Create directories if they do not exist\n",
        "    os.makedirs(train_class_dir, exist_ok=True)\n",
        "    os.makedirs(val_class_dir, exist_ok=True)\n",
        "    os.makedirs(test_class_dir, exist_ok=True)\n",
        "\n",
        "    # Copy images\n",
        "    for img in train_images:\n",
        "        shutil.copy(img, train_class_dir)\n",
        "\n",
        "    for img in val_images:\n",
        "        shutil.copy(img, val_class_dir)\n",
        "\n",
        "    for img in test_images:\n",
        "        shutil.copy(img, test_class_dir)\n",
        "\n",
        "    # Return one image from train, val, and test for display\n",
        "    return train_images[0], val_images[0], test_images[0]\n",
        "\n",
        "# Dictionaries to store image paths for each class\n",
        "train_sample_images = {}\n",
        "val_sample_images = {}\n",
        "test_sample_images = {}\n",
        "\n",
        "# Copy images for each class and get a sample image from each class\n",
        "for class_name in classes:\n",
        "    src_dir = os.path.join(original_dataset_dir, class_name)\n",
        "    train_img, val_img, test_img = copy_images(class_name, src_dir, train_dir, val_dir, test_dir)\n",
        "    train_sample_images[class_name] = train_img\n",
        "    val_sample_images[class_name] = val_img\n",
        "    test_sample_images[class_name] = test_img\n",
        "\n",
        "print(\"Dataset reorganized successfully\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-jPKQjO5s6KD",
        "outputId": "d3adfc7f-7c3e-44a8-b714-c9fedbfe22fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset reorganized successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf \"BreastUltraSoundImages\""
      ],
      "metadata": {
        "id": "x4Q1L8Aqzlx7"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the paths\n",
        "original_dataset_dir = 'Dataset_BUSI_with_GT'\n",
        "base_dir = 'BreastUltraSoundImages'\n",
        "\n",
        "# Define new paths for train, val, and test\n",
        "image_dir = os.path.join(base_dir, 'images')\n",
        "label_dir = os.path.join(base_dir, 'labels')\n",
        "train_image_dir = os.path.join(image_dir, 'train')\n",
        "val_image_dir = os.path.join(image_dir, 'val')\n",
        "test_image_dir = os.path.join(image_dir, 'test')\n",
        "train_label_dir = os.path.join(label_dir, 'train')\n",
        "val_label_dir = os.path.join(label_dir, 'val')\n",
        "test_label_dir = os.path.join(label_dir, 'test')\n",
        "\n",
        "# Create directories if they do not exist\n",
        "os.makedirs(train_image_dir, exist_ok=True)\n",
        "os.makedirs(val_image_dir, exist_ok=True)\n",
        "os.makedirs(test_image_dir, exist_ok=True)\n",
        "os.makedirs(train_label_dir, exist_ok=True)\n",
        "os.makedirs(val_label_dir, exist_ok=True)\n",
        "os.makedirs(test_label_dir, exist_ok=True)\n",
        "\n",
        "# List the classes\n",
        "classes = ['benign', 'malignant', 'normal']\n",
        "\n",
        "# Function to copy images and corresponding labels\n",
        "def copy_images_and_labels(class_name, src_image_dir, src_label_dir, train_image_dst, val_image_dst, test_image_dst,\n",
        "                           train_label_dst, val_label_dst, test_label_dst, train_ratio=0.7, val_ratio=0.15):\n",
        "    # List all images in the class\n",
        "    images = os.listdir(src_image_dir)\n",
        "    images = [img for img in images if img.endswith('.png')]  # Adjust the extension if needed\n",
        "    image_paths = [os.path.join(src_image_dir, img) for img in images]\n",
        "    label_paths = [os.path.join(src_label_dir, img.replace('.png', '_mask.png')) for img in images]  # Adjust the mask naming convention\n",
        "\n",
        "    # Split into train, val, and test\n",
        "    train_images, temp_images, train_labels, temp_labels = train_test_split(\n",
        "        image_paths, label_paths, train_size=train_ratio, random_state=42)\n",
        "    val_images, test_images, val_labels, test_labels = train_test_split(\n",
        "        temp_images, temp_labels, test_size=val_ratio / (val_ratio + (1 - train_ratio)), random_state=42)\n",
        "\n",
        "    # Create class directories in train, val, and test\n",
        "    train_image_class_dir = os.path.join(train_image_dst, class_name)\n",
        "    val_image_class_dir = os.path.join(val_image_dst, class_name)\n",
        "    test_image_class_dir = os.path.join(test_image_dst, class_name)\n",
        "    train_label_class_dir = os.path.join(train_label_dst, class_name)\n",
        "    val_label_class_dir = os.path.join(val_label_dst, class_name)\n",
        "    test_label_class_dir = os.path.join(test_label_dst, class_name)\n",
        "\n",
        "    # Create directories if they do not exist\n",
        "    os.makedirs(train_image_class_dir, exist_ok=True)\n",
        "    os.makedirs(val_image_class_dir, exist_ok=True)\n",
        "    os.makedirs(test_image_class_dir, exist_ok=True)\n",
        "    os.makedirs(train_label_class_dir, exist_ok=True)\n",
        "    os.makedirs(val_label_class_dir, exist_ok=True)\n",
        "    os.makedirs(test_label_class_dir, exist_ok=True)\n",
        "\n",
        "    # Copy images and labels\n",
        "    for img, label in zip(train_images, train_labels):\n",
        "        if os.path.exists(label):  # Ensure the label file exists before copying\n",
        "            shutil.copy(img, train_image_class_dir)\n",
        "            shutil.copy(label, train_label_class_dir)\n",
        "\n",
        "    for img, label in zip(val_images, val_labels):\n",
        "        if os.path.exists(label):  # Ensure the label file exists before copying\n",
        "            shutil.copy(img, val_image_class_dir)\n",
        "            shutil.copy(label, val_label_class_dir)\n",
        "\n",
        "    for img, label in zip(test_images, test_labels):\n",
        "        if os.path.exists(label):  # Ensure the label file exists before copying\n",
        "            shutil.copy(img, test_image_class_dir)\n",
        "            shutil.copy(label, test_label_class_dir)\n",
        "\n",
        "# Copy images and labels for each class\n",
        "for class_name in classes:\n",
        "    src_image_dir = os.path.join(original_dataset_dir, class_name)\n",
        "    src_label_dir = os.path.join(original_dataset_dir, class_name)  # Adjust if masks are in a different directory\n",
        "    copy_images_and_labels(class_name, src_image_dir, src_label_dir, train_image_dir, val_image_dir, test_image_dir,\n",
        "                           train_label_dir, val_label_dir, test_label_dir)\n",
        "\n",
        "print(\"Dataset reorganized successfully\")\n"
      ],
      "metadata": {
        "id": "FtJHJu0d0iOv",
        "outputId": "ceceb1f8-da86-4945-bc43-98669fdd36ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset reorganized successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Define the paths\n",
        "base_dir = 'BreastUltraSoundImages'\n",
        "image_dir = os.path.join(base_dir, 'images')\n",
        "label_dir = os.path.join(base_dir, 'labels')\n",
        "\n",
        "# List the classes and assign them IDs\n",
        "classes = ['benign', 'malignant', 'normal']\n",
        "class_id_mapping = {cls_name: i for i, cls_name in enumerate(classes)}\n",
        "\n",
        "# Function to convert bounding box to YOLO format\n",
        "def convert_to_yolo_format(size, box):\n",
        "    dw = 1. / size[0]\n",
        "    dh = 1. / size[1]\n",
        "    x_center = (box[0] + box[1]) / 2.0\n",
        "    y_center = (box[2] + box[3]) / 2.0\n",
        "    width = box[1] - box[0]\n",
        "    height = box[3] - box[2]\n",
        "    x_center *= dw\n",
        "    width *= dw\n",
        "    y_center *= dh\n",
        "    height *= dh\n",
        "    return (x_center, y_center, width, height)\n",
        "\n",
        "# Function to create annotation files for YOLO\n",
        "def create_yolo_annotations(class_name, src_label_dir, img_dir):\n",
        "    label_files = [f for f in os.listdir(src_label_dir) if f.endswith('_mask.png')]\n",
        "    class_id = class_id_mapping[class_name]\n",
        "\n",
        "    for label_file in label_files:\n",
        "        image_file = label_file.replace('_mask.png', '.png')\n",
        "        img_path = os.path.join(img_dir, image_file)\n",
        "        label_path = os.path.join(src_label_dir, label_file)\n",
        "\n",
        "        # Check if the image file exists\n",
        "        if not os.path.exists(img_path):\n",
        "            print(f\"Warning: Image file {img_path} not found.\")\n",
        "            continue\n",
        "\n",
        "        # Load the mask and find contours\n",
        "        mask = cv2.imread(label_path, 0)\n",
        "        if mask is None:\n",
        "            print(f\"Warning: Mask file {label_path} not found or could not be loaded.\")\n",
        "            continue\n",
        "\n",
        "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        # Load the image to get its dimensions\n",
        "        img = Image.open(img_path)\n",
        "        w, h = img.size\n",
        "\n",
        "        # Create annotation file for YOLO\n",
        "        yolo_annotation = []\n",
        "        if contours:\n",
        "            for contour in contours:\n",
        "                x, y, w_box, h_box = cv2.boundingRect(contour)\n",
        "                box = (x, x+w_box, y, y+h_box)\n",
        "                yolo_box = convert_to_yolo_format((w, h), box)\n",
        "                yolo_annotation.append(f\"{class_id} {' '.join(map(str, yolo_box))}\\n\")\n",
        "        else:\n",
        "            # If there are no contours, create an empty annotation file\n",
        "            yolo_annotation.append(\"\")\n",
        "\n",
        "        # Write annotations to file in the same directory as the mask\n",
        "        annotation_file = label_path.replace('_mask.png', '.txt')\n",
        "        with open(annotation_file, 'w') as f:\n",
        "            f.writelines(yolo_annotation)\n",
        "        #print(f\"Created annotation file: {annotation_file}\")\n",
        "\n",
        "        # Remove the mask file after creating the annotation\n",
        "        os.remove(label_path)\n",
        "        #print(f\"Removed mask file: {label_path}\")\n",
        "\n",
        "# Process each class and its subdirectories\n",
        "for class_name in classes:\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        src_label_dir = os.path.join(label_dir, split, class_name)\n",
        "        img_dir = os.path.join(image_dir, split, class_name)\n",
        "\n",
        "        if os.path.exists(src_label_dir) and os.path.exists(img_dir):\n",
        "            create_yolo_annotations(class_name, src_label_dir, img_dir)\n",
        "        else:\n",
        "            if not os.path.exists(src_label_dir):\n",
        "                print(f\"Label directory for class {class_name} in {split} not found: {src_label_dir}\")\n",
        "            if not os.path.exists(img_dir):\n",
        "                print(f\"Image directory for class {class_name} in {split} not found: {img_dir}\")\n",
        "\n",
        "print(\"Annotations creation process completed.\")\n"
      ],
      "metadata": {
        "id": "1KfXUhtZDmSF",
        "outputId": "5f3f2dc2-c33e-498d-874e-9c303740a798",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Annotations creation process completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the paths\n",
        "base_dir = 'BreastUltraSoundImages'\n",
        "image_dir = os.path.join(base_dir, 'images')\n",
        "label_dir = os.path.join(base_dir, 'labels')\n",
        "\n",
        "# List the classes\n",
        "classes = ['benign', 'malignant', 'normal']\n",
        "splits = ['train', 'val', 'test']\n",
        "\n",
        "# Function to check for missing annotation files\n",
        "def check_annotations():\n",
        "    for split in splits:\n",
        "        for class_name in classes:\n",
        "            img_subdir = os.path.join(image_dir, split, class_name)\n",
        "            lbl_subdir = os.path.join(label_dir, split, class_name)\n",
        "\n",
        "            # Check if the image and label directories exist\n",
        "            if not os.path.exists(img_subdir):\n",
        "                print(f\"Image directory not found: {img_subdir}\")\n",
        "                continue\n",
        "            if not os.path.exists(lbl_subdir):\n",
        "                print(f\"Label directory not found: {lbl_subdir}\")\n",
        "                continue\n",
        "\n",
        "            # List all images and labels\n",
        "            image_files = [f for f in os.listdir(img_subdir) if f.endswith('.png')]\n",
        "            label_files = [f for f in os.listdir(lbl_subdir) if f.endswith('.txt')]\n",
        "\n",
        "            # Create sets of file names (excluding extensions)\n",
        "            image_files_set = {os.path.splitext(f)[0] for f in image_files}\n",
        "            label_files_set = {os.path.splitext(f)[0] for f in label_files}\n",
        "\n",
        "            # Check for missing annotation files\n",
        "            missing_labels = image_files_set - label_files_set\n",
        "            if missing_labels:\n",
        "                print(f\"Missing labels for the following images in {split}/{class_name}:\")\n",
        "                for missing in missing_labels:\n",
        "                    print(f\"  {missing}.png\")\n",
        "\n",
        "# Run the check\n",
        "check_annotations()\n"
      ],
      "metadata": {
        "id": "9S_vD5CBCIJV"
      },
      "execution_count": 50,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}